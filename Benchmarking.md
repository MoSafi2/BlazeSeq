# FASTQ Parser Benchmarking

This document describes the benchmarking harness that compares BlazeSeq to other FASTQ parsers: **needletail** (Rust), **seq_io** (Rust), **kseq** (C), and **FASTX.jl** (Julia).

## Purpose

The benchmark measures **parsing throughput** (time to read and iterate over all records in a large FASTQ file) so that BlazeSeq can be compared fairly to established parsers. Each implementation performs equivalent work: open the file, parse every record, and count records and base pairs.

## Methodology

- **Data**: A single synthetic FASTQ file of **3 GB** is generated with the same format used elsewhere in BlazeSeq: 4-line records, header `@read_<index>`, read length 100 bp, Sanger-like quality (generic schema).
- **Location**: The file is written to a **tmpfs (Linux)** mount (or, if mount is not possible, to `/dev/shm`). All reads during the benchmark therefore come from RAM, not from disk.
- **Work**: Each parser is run repeatedly by [hyperfine](https://github.com/sharkdp/hyperfine). Each run opens the file, iterates over all records, and sums record count and sequence lengths. Output is a single line `records base_pairs` for verification.

## Minimizing I/O Overhead (tmpfs / ramfs)

Hard drives and SSDs introduce variable latency and throughput limits. To measure **parser CPU throughput** rather than storage speed, the benchmark file must live in RAM.

**Behavior of the harness:**

1. A temporary directory is created (`mktemp -d`).
2. On **Linux**, a tmpfs is mounted on that directory (`mount -t tmpfs -o size=5G tmpfs <dir>`). The 3 GB synthetic file is written there. All reads during hyperfine then hit RAM.
3. The 3 GB file is generated by the Mojo script `benchmark/fastq-parser/generate_synthetic_fastq.mojo` and written into that directory.
4. All parsers are run against the same file path.
5. After the benchmark, the tmpfs is unmounted and the directory is removed.

**Requirements:**

- On Linux, **sudo** is required for `mount` and `umount`. If sudo is not available, the script falls back to a directory under `/dev/shm` (already tmpfs), so no mount is needed.
- On **macOS**, the script does not create a ramdisk by default; it uses a normal temporary directory. For a ramdisk on macOS you can create one manually (e.g. with `diskutil` and `hdiutil`) and set the benchmark file path accordingly, or see your systemâ€™s documentation for RAM disk setup.

## Parameters

| Parameter        | Value |
|-----------------|--------|
| File size       | 3 GB  |
| Read length     | 100 bp (min=max=100) |
| Quality schema  | generic (Sanger-like) |
| hyperfine warmup| 2     |
| hyperfine runs  | 5     |
| hyperfine output| Markdown and JSON in repo root |

## Prerequisites

Install the following so that `benchmark/fastq-parser/run_benchmarks.sh` can run:

| Tool      | Purpose              | Install |
|-----------|----------------------|--------|
| pixi      | Run Mojo (BlazeSeq)  | [pixi.sh](https://pixi.sh) |
| hyperfine | Timed runs           | [sharkdp/hyperfine](https://github.com/sharkdp/hyperfine) |
| Rust      | needletail, seq_io   | [rustup.rs](https://rustup.rs) |
| C compiler| kseq (plain C)       | gcc or clang |
| Julia     | FASTX.jl             | [julialang.org](https://julialang.org) |

For the tmpfs workflow on Linux, **sudo** is needed for mount/umount (or use the `/dev/shm` fallback).

## Installing toolchains (recommended)

From the **repository root**, run the install script to install missing tools into the pixi **benchmark** environment (hyperfine, Rust, Julia from conda-forge). It will also install pixi if needed. The C compiler (gcc or clang) is not provided by the env; the script will tell you how to install it on your OS if missing.

```bash
./benchmark/fastq-parser/install_toolchains.sh
```

Then run benchmarks using the benchmark environment so the correct tools are on PATH:

```bash
pixi run -e benchmark ./benchmark/fastq-parser/run_benchmarks.sh
# or
pixi run -e benchmark benchmark
```

## How to Run

From the **repository root**, after installing toolchains (see above):

```bash
pixi run -e benchmark ./benchmark/fastq-parser/run_benchmarks.sh
```

If all tools (pixi, hyperfine, cargo, rustc, gcc or clang, julia) are already on your PATH, you can run:

```bash
./benchmark/fastq-parser/run_benchmarks.sh
```

On Linux, if the script mounts a tmpfs, you will be prompted for sudo for mount and again for umount at the end.

Results are written to:

- `benchmark_results.md` (summary table)
- `benchmark_results.json` (full hyperfine output)

## Parser Versions

Record the versions you use when reporting results, for example:

- **BlazeSeq**: Mojo version from `pixi run mojo --version`
- **needletail**: `benchmark/fastq-parser/needletail_runner/Cargo.toml` (e.g. 0.6.x)
- **seq_io**: `benchmark/fastq-parser/seq_io_runner/Cargo.toml` (e.g. 0.3.x)
- **kseq**: vendored from [lh3/seqtk](https://github.com/lh3/seqtk) (kseq.h)
- **FASTX.jl**: `benchmark/fastq-parser/Project.toml` (FASTX dependency)

## Interpreting Results

- **Time**: hyperfine reports mean and median run time (e.g. in seconds). Lower is better.
- **Throughput**: For a 3 GB file, throughput in GB/s is `3.0 / time_seconds`. Use this to compare parsers across machines.

All parsers are run on the same file; the script checks that they report the same `records` and `base_pairs` before running hyperfine, so any difference in time is attributable to parsing and iteration rather than I/O or correctness.
