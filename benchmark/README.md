# BlazeSeq Benchmarks

This document describes all benchmarking options for BlazeSeq. Benchmark numbers are hardware- and Mojo-version-dependent.

## Quick reference

| Benchmark | Command | Description |
|-----------|---------|-------------|
| **BlazeSeq throughput** (file) | `pixi run -e benchmark benchmark-throughput` | ~3 GB on tmpfs/ramfs, batches/records/ref_records with hyperfine |
| **BlazeSeq throughput** (memory) | `pixi run -e benchmark benchmark-throughput-memory` | In-process FASTQ, parse-only timing, JSON + plots |
| **Parser comparison** (plain) | `pixi run -e benchmark benchmark-plain` | BlazeSeq vs needletail, seq_io, kseq on 3 GB plain FASTQ |
| **Parser comparison** (gzip) | `pixi run -e benchmark benchmark-gzip` | Decompress + parse; BlazeSeq multi-threaded (default 4 threads) |
| **Parser comparison** (gzip, 1 thread) | `pixi run -e benchmark benchmark-gzip-single` | Fair single-threaded comparison |
| **Plot results** | `pixi run -e benchmark benchmark-plot` | Generate PNGs from JSON in `assets/` |

## Prerequisites

| Tool | Purpose | Install |
|------|---------|--------|
| pixi | Mojo, hyperfine, Rust | See [Install pixi](#install-pixi) below |
| C compiler | kseq (plain C) | gcc or clang (system; not in pixi) |

The pixi **benchmark** environment supplies Mojo, hyperfine, and Rust. Install a C compiler yourself for the kseq benchmark.

On Linux, **sudo** is needed for ramfs/tmpfs mount/umount (or the script uses `/dev/shm`). For strict CPU timing (performance governor, disable turbo), **sudo** may be required for `cpupower` and `/sys/.../intel_pstate/no_turbo`.

### Mount type: `--ramfs` or `--tmpfs`

File-based benchmark scripts (plain comparison, gzip comparison, throughput) accept an optional mount type so the synthetic FASTQ lives in RAM:

- **`--ramfs`** — Use a ramfs mount (no size limit, no swap). This is the **default** and what the pixi benchmark tasks use.
- **`--tmpfs`** — Use a tmpfs mount with a 5 GB size limit (`-o size=5G`).

Examples (from repository root):

```bash
./benchmark/fastq-parser/run_benchmarks.sh --ramfs
./benchmark/fastq-parser/run_benchmarks.sh --tmpfs
./benchmark/fastq-parser/run_benchmarks_gzip.sh --ramfs 8
./benchmark/throughput/run_throughput_benchmarks.sh --tmpfs
```

The pixi tasks `benchmark-plain`, `benchmark-gzip`, `benchmark-gzip-single`, and `benchmark-throughput` all pass **`--ramfs`** by default.

### Install pixi

```bash
curl -fsSL https://pixi.sh/install.sh | bash
```

Ensure `~/.local/bin` is on your `PATH`. From the **repository root**:

```bash
pixi install -e benchmark
```

---

## 1. BlazeSeq-only throughput

### File-based (hyperfine)

Generates ~3 GB synthetic FASTQ on tmpfs/ramfs, then runs batches / records / ref_records with hyperfine:

```bash
pixi run -e benchmark benchmark-throughput
```

### In-memory (MemoryReader)

Generates ~3 GB FASTQ in process (no disk I/O), reads via `MemoryReader`. Timing is measured inside Mojo (parse-only); the script runs each mode multiple times, captures `parse_seconds`, and writes JSON + plots (no hyperfine):

```bash
pixi run -e benchmark benchmark-throughput-memory
```

- Override size and runs: `SIZE_GB=1 BENCH_RUNS=3 ./benchmark/throughput/run_throughput_memory_benchmarks.sh`
- Run once (default 3 GB): `pixi run mojo run -I . benchmark/throughput/run_throughput_memory_blazeseq.mojo [size_gb] <mode>` (mode: `batches` | `records` | `ref_records`).

---

## 2. Parser comparison (plain FASTQ)

Compares BlazeSeq to **needletail** (Rust), **seq_io** (Rust), and **kseq** (C).

- **Purpose**: Parsing throughput (time to read and iterate over all records). Each implementation counts records and base pairs.
- **Data**: A single **3 GB** synthetic FASTQ (4-line records, read length 100 bp, generic quality). Generated by `benchmark/fastq-parser/generate_synthetic_fastq.mojo`.
- **Location**: File is written to a **tmpfs** (Linux) or `/dev/shm` so reads come from RAM, not disk.

From the **repository root**:

```bash
pixi run -e benchmark benchmark-plain
```

Or run the script directly:

```bash
pixi run -e benchmark ./benchmark/fastq-parser/run_benchmarks.sh
```

Results: `benchmark_results.md`, `benchmark_results.json`. Ensure at least ~5 GB RAM for the 3 GB file and tmpfs.

---

## 3. Parser comparison (gzip FASTQ)

Compares **decompress + parse** across kseq (C + zlib), seq_io (Rust, flate2), needletail (Rust), and BlazeSeq (RapidgzipReader).

- **Data**: Same 3 GB synthetic FASTQ, compressed with `gzip` to a single `.fastq.gz`; only the compressed file is used during hyperfine.
- **Output**: Separate from plain benchmark — `benchmark_results_gzip.md` / `benchmark_results_gzip.json` (multi-threaded) or `benchmark_results_gzip_single.md` / `.json` (single-threaded).

### Decompression concurrency

| Parser | Backend | Concurrency |
|--------|---------|-------------|
| kseq, needletail, seq_io | zlib / flate2 | **Single-threaded** |
| BlazeSeq | rapidgzip | **Multi-threaded** (default 4 threads; override with script arg or `GZIP_BLAZESEQ_THREADS`) |

So the default gzip benchmark is **multi-threaded (BlazeSeq) vs single-threaded (others)**. For a fair single-threaded comparison, use the single-threaded gzip benchmark.

### Run gzip benchmark

**Multi-threaded** (BlazeSeq default 4 threads):

```bash
pixi run -e benchmark benchmark-gzip
pixi run -e benchmark benchmark-gzip 8   # 8 threads
pixi run -e benchmark benchmark-gzip 0   # all available threads
```

**Single-threaded** (BlazeSeq 1 thread; all tools pinned to one core):

```bash
pixi run -e benchmark benchmark-gzip-single
```

Or: `GZIP_BENCH_PARALLELISM=1 ./benchmark/fastq-parser/run_benchmarks_gzip.sh`

---

## Methodology (tmpfs / ramfs)

To measure **parser CPU throughput** rather than storage speed, the benchmark file lives in RAM.

1. A temporary directory is created (`mktemp -d`).
2. On **Linux**, either **ramfs** or **tmpfs** is mounted on that directory, depending on the script option: `--ramfs` (default; pixi uses this) or `--tmpfs` (tmpfs with `-o size=5G`). The synthetic file is written there.
3. The file is generated by `benchmark/fastq-parser/generate_synthetic_fastq.mojo`.
4. Parsers run against the same file path; after the benchmark, the mount is unmounted.

If sudo is not available on Linux, the script falls back to a directory under `/dev/shm`. On **macOS**, a normal temporary directory is used unless you set up a ramdisk manually.

### Parameters

| Parameter | Value |
|-----------|--------|
| File size | 3 GB |
| Read length | 100 bp |
| Quality schema | generic (Sanger-like) |
| hyperfine warmup | 3 |
| hyperfine runs | 15 |
| hyperfine output | Markdown and JSON (repo root or as noted) |

---

## CPU benchmark environment (Linux)

To reduce timing variation (frequency scaling and turbo can cause ~5–15% variation):

1. **Performance governor**: `cpupower frequency-set -g performance` (requires `cpupower`, e.g. `linux-tools-common`).
2. **Disable turbo** (Intel): `echo 1 | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo`. Not applied on AMD or if the path is missing.
3. **Pin to cores**: Hyperfine runs under `taskset -c "$BENCH_CPUS"` (default core `0`). Override with `BENCH_CPUS=0-3` etc.

Governor and turbo state are **restored on exit**. If `cpupower` or `taskset` is missing, the scripts still run but skip the corresponding step.

---

## Plotting results

After any benchmark, generate **column plots with error bars** from the hyperfine JSON:

- **Script**: `benchmark/scripts/plot_benchmark_results.py`
- **Output**: PNGs in `assets/` (e.g. `parser_plain.png`, `parser_gzip.png`, `throughput.png`, `throughput_gbps.png`).

Plotting runs automatically after hyperfine when Python and matplotlib are available. To plot existing results without re-running:

```bash
pixi run -e benchmark benchmark-plot
```

Optional args for the plot script: `--runs`, `--size-gb`, `--reads`, and for gzip `--threads`:

```bash
pixi run -e benchmark python benchmark/scripts/plot_benchmark_results.py --repo-root . --assets-dir assets --json benchmark_results.json
```

---

## Interpreting results

- **Time**: hyperfine reports mean/median run time (seconds). Lower is better.
- **Throughput**: For a 3 GB file, GB/s = `3.0 / time_seconds`.
- The script checks that all parsers report the same `records` and `base_pairs`; differences in time are from parsing/iteration, not I/O or correctness.

### Parser versions to report

- **BlazeSeq**: `pixi run mojo --version`
- **needletail**: `benchmark/fastq-parser/needletail_runner/Cargo.toml`
- **seq_io**: `benchmark/fastq-parser/seq_io_runner/Cargo.toml`
- **kseq**: vendored from [lh3/seqtk](https://github.com/lh3/seqtk) (kseq.h)
